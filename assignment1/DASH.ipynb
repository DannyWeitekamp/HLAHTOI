{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between DASH and ACTR\n",
    "See the rest here: https://github.com/DannyWeitekamp/HLAHTOI/tree/master/assignment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_learner_history(now,times,correct,alpha=0.0,delta=0.0,windows=[1/24,1,7,30]):\n",
    "    times = np.array(times)\n",
    "    correct = np.array(correct)\n",
    "    elapse = now-times\n",
    "    windows = [0.0] + [86400.0*x for x in windows] + [float('inf')]\n",
    "    lh = {}\n",
    "    for i in range(len(windows)-1):\n",
    "        upper = windows[i+1]\n",
    "        lower = windows[i]\n",
    "        in_win = np.logical_and(elapse > lower,elapse <= upper)\n",
    "        c_in_win = in_win * correct\n",
    "        \n",
    "        lh['n_w'+str(i)] = in_win.sum()\n",
    "        lh['c_w'+str(i)] = c_in_win.sum()\n",
    "    lh[\"elapses\"] = elapse\n",
    "    lh[\"alpha\"] = alpha\n",
    "    lh[\"delta\"] = delta\n",
    "    return lh\n",
    "\n",
    "def day(x):\n",
    "    return x * 86400.0\n",
    "def hour(x):\n",
    "    return x * 3600\n",
    "def mins(x):\n",
    "    return x * 60\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting The DASH Model\n",
    "\n",
    "Because DASH doesn't come with a description of parameter values, I had to fit a model to data. I used some data that Theo used in his study; preprocessed data from the assistments17 dataset. See the other two notebooks in this github for how I processed the data further for this purpose. I tried fitting the data with pyTorch, but it didn't converge, so I tried again by using with the glmer package in R (this one worked better). I made a phi and psi fixed effects and had random effects for the student and kc intercepts (I also did not use a global intercept). I include the fit parameters for both models here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Fit with torch---\n",
    "#phi tensor([-0.1015,  0.1419,  0.1439,  0.1933,  0.0000])\n",
    "#psi tensor([-0.1941,  0.1973,  0.0245,  0.0347,  0.0000])\n",
    "\n",
    "#---Fit with glmer (in R)----\n",
    "#    ltc_0      lop_0      ltc_1      lop_1      ltc_2      lop_2      ltc_3      lop_3      ltc_4  lop_4  \n",
    "# 0.038408  -0.011849   0.048038  -0.032408   0.033345  -0.024711  -0.003275   0.005272  -0.043829  0.033895  \n",
    "    \n",
    " \n",
    "window_profiles = [{\n",
    "    'scale' : 1/24,\n",
    "    'phi' : .038408,\n",
    "    'psi' : -0.011849\n",
    "},\n",
    "{\n",
    "    'scale' : 1,\n",
    "    'phi' : 0.048038,\n",
    "    'psi' : -0.032408\n",
    "},\n",
    "{\n",
    "    'scale' : 7,\n",
    "    'phi' : 0.033345,\n",
    "    'psi' : -0.024711\n",
    "},\n",
    "{\n",
    "    'scale' : 30,\n",
    "    'phi' : -0.003275,\n",
    "    'psi' : 0.005272\n",
    "},\n",
    "]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x)) \n",
    "\n",
    "def DASH_activation(lh,wps):\n",
    "    s = 0\n",
    "    for i,wp in enumerate(wps):\n",
    "        s += wp[\"phi\"]*np.log(1+lh[\"c_w\"+str(i)])-wp[\"psi\"]*np.log(1+lh[\"n_w\"+str(i)])\n",
    "    #print(s*10)\n",
    "    s*=10\n",
    "    return lh['alpha'] - lh['delta'] + s\n",
    "\n",
    "def DASH(lh,wps):\n",
    "    x = DASH_activation(lh,wps)\n",
    "    return 1.0/(1 + np.exp(-x)) \n",
    "\n",
    "def whitehill_activation(lh):\n",
    "    B_t = (lh['alpha'] - lh['delta']) #Best Guess\n",
    "    return np.log(np.sum(np.power(lh['elapses']/3600.0,-.5)))+B_t\n",
    "\n",
    "def whitehill(lh):\n",
    "    x = whitehill_activation(lh)\n",
    "    return 1.0/(1 + np.exp(-x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between DASH and Whitehill Implementation of(ACT-R)\n",
    "\n",
    "I compare predictions of DASH vs Whitehill's ACT-R implementation on three different studying patterns:\n",
    "1. Crammed: A little bit of study 2 days before, lots of study 30 minutes before the test\n",
    "2. Spaced: Study sessions are spaced over different days\n",
    "3. Massed: Lots of Study 2 days before the test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cram_h = [\n",
    "    day(0)+hour(0)+mins(1),\n",
    "    day(0)+hour(0)+mins(2),\n",
    "    day(0)+hour(0)+mins(3),\n",
    "    day(2)+hour(4)+mins(31),\n",
    "    day(2)+hour(4)+mins(32),\n",
    "    day(2)+hour(4)+mins(33),\n",
    "    day(2)+hour(4)+mins(34),\n",
    "    day(2)+hour(4)+mins(35),\n",
    "    day(2)+hour(4)+mins(36),\n",
    "    day(2)+hour(4)+mins(37),\n",
    "    day(2)+hour(4)+mins(38),\n",
    "]\n",
    "cram_c = [0,0,1,0,0,1,1,0,1,1,1]\n",
    "\n",
    "\n",
    "spaced_h = [\n",
    "    day(0)+hour(0)+mins(1),\n",
    "    day(0)+hour(0)+mins(2),\n",
    "    day(0)+hour(0)+mins(3),\n",
    "    day(1)+hour(4)+mins(31),\n",
    "    day(1)+hour(5)+mins(32),\n",
    "    day(1)+hour(6)+mins(33),\n",
    "    day(1)+hour(7)+mins(34),\n",
    "    day(2)+hour(1)+mins(35),\n",
    "    day(2)+hour(2)+mins(36),\n",
    "    day(2)+hour(3)+mins(37),\n",
    "    day(2)+hour(4)+mins(38),\n",
    "]\n",
    "spaced_c = [0,0,1,0,0,1,1,0,1,1,1]\n",
    "\n",
    "massed_h = [\n",
    "    day(0)+hour(0)+mins(1),\n",
    "    day(0)+hour(0)+mins(2),\n",
    "    day(0)+hour(0)+mins(3),\n",
    "    day(0)+hour(1)+mins(31),\n",
    "    day(0)+hour(1)+mins(32),\n",
    "    day(0)+hour(1)+mins(33),\n",
    "    day(0)+hour(1)+mins(34),\n",
    "    day(0)+hour(1)+mins(35),\n",
    "    day(0)+hour(1)+mins(36),\n",
    "    day(0)+hour(1)+mins(37),\n",
    "    day(0)+hour(1)+mins(38),\n",
    "]\n",
    "massed_c = [0,0,1,0,0,1,1,0,1,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Interestingly ACT-R predicts that cramming is the best, but DASH predicts that spacing is the best. Values are printed out here as probabilities (from 0 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DASH Predictions\n",
      "Crammed\t: 0.8133953765694286\n",
      "Spaced\t: 0.9071236738735295\n",
      "Massed\t: 0.7708132621017548\n",
      "---------------------------------\n",
      "Whitehill Predictions\n",
      "Crammed\t: 0.922107772884626\n",
      "Spaced\t: 0.7952508758069727\n",
      "Massed\t: 0.32235036724594934\n"
     ]
    }
   ],
   "source": [
    "now = day(2)+hour(5)\n",
    "cram_lh = gen_learner_history(now,cram_h,cram_c,alpha=.05,delta=.1)\n",
    "spaced_lh = gen_learner_history(now,spaced_h,spaced_c,alpha=.05,delta=.1)\n",
    "massed_lh = gen_learner_history(now,massed_h,massed_c,alpha=.05,delta=.1) \n",
    "\n",
    "print(\"DASH Predictions\")\n",
    "print(\"Crammed\\t:\", DASH(cram_lh,window_profiles))\n",
    "print(\"Spaced\\t:\", DASH(spaced_lh,window_profiles))\n",
    "print(\"Massed\\t:\", DASH(massed_lh,window_profiles))\n",
    "print(\"---------------------------------\")\n",
    "print(\"Whitehill Predictions\")\n",
    "print(\"Crammed\\t:\", whitehill(cram_lh))\n",
    "print(\"Spaced\\t:\", whitehill(spaced_lh))\n",
    "print(\"Massed\\t:\", whitehill(massed_lh))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
