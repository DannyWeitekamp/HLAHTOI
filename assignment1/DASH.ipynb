{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between DASH and ACTR\n",
    "See the rest here: https://github.com/DannyWeitekamp/HLAHTOI/tree/master/assignment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_learner_history(now,times,correct,alpha=0.0,delta=0.0,windows=[1/24,1,7,30]):\n",
    "    times = np.array(times)\n",
    "    correct = np.array(correct)\n",
    "    elapse = now-times\n",
    "    windows = [0.0] + [86400.0*x for x in windows] + [float('inf')]\n",
    "    lh = {}\n",
    "    for i in range(len(windows)-1):\n",
    "        upper = windows[i+1]\n",
    "        lower = windows[i]\n",
    "        in_win = np.logical_and(elapse > lower,elapse <= upper)\n",
    "        c_in_win = in_win * correct\n",
    "        \n",
    "        lh['n_w'+str(i)] = in_win.sum()\n",
    "        lh['c_w'+str(i)] = c_in_win.sum()\n",
    "    lh[\"elapses\"] = elapse\n",
    "    lh[\"alpha\"] = alpha\n",
    "    lh[\"delta\"] = delta\n",
    "    return lh\n",
    "\n",
    "def day(x):\n",
    "    return x * 86400.0\n",
    "def hour(x):\n",
    "    return x * 3600\n",
    "def mins(x):\n",
    "    return x * 60\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting The DASH Model\n",
    "\n",
    "Because DASH doesn't come with a description of parameter values, I had to fit a model to data. I used some data that Theo used in his study; preprocessed data from the assistments17 dataset. See the other two notebooks in this github for how I processed the data further for this purpose. I tried fitting the data with pyTorch, but it didn't converge, so I tried again by using with the glmer package in R (this one worked better). I made a phi and psi fixed effects and had random effects for the student and kc intercepts (I also did not use a global intercept). I include the fit parameters for both models here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Fit with torch---\n",
    "#phi tensor([-0.1015,  0.1419,  0.1439,  0.1933,  0.0000])\n",
    "#psi tensor([-0.1941,  0.1973,  0.0245,  0.0347,  0.0000])\n",
    "\n",
    "#---Fit with glmer (in R)----\n",
    "#    ltc_0      lop_0      ltc_1      lop_1      ltc_2      lop_2      ltc_3      lop_3      ltc_4    lop_4  \n",
    "#  0.038803  -0.011770   0.041937  -0.029086   0.060719  -0.038129   0.033654  -0.028097   0.010040  -0.004177  \n",
    "    \n",
    " \n",
    "window_profiles = [{\n",
    "    'scale' : 1/24,\n",
    "    'phi' :  0.038803,\n",
    "    'psi' : -0.011770\n",
    "},\n",
    "{\n",
    "    'scale' : 1,\n",
    "    'phi' :  0.041937,\n",
    "    'psi' : -0.029086\n",
    "},\n",
    "{\n",
    "    'scale' : 7,\n",
    "    'phi' : 0.060719,\n",
    "    'psi' : -0.038129\n",
    "},\n",
    "{\n",
    "    'scale' : 30,\n",
    "    'phi' : 0.033654,\n",
    "    'psi' : -0.028097\n",
    "},\n",
    "]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x)) \n",
    "\n",
    "def DASH_activation(lh,wps):\n",
    "    s = 0\n",
    "    for i,wp in enumerate(wps):\n",
    "        s += wp[\"phi\"]*np.log(1+lh[\"c_w\"+str(i)])-wp[\"psi\"]*np.log(1+lh[\"n_w\"+str(i)])\n",
    "    #print(s*10)\n",
    "    s*=10\n",
    "    return lh['alpha'] - lh['delta'] + s\n",
    "\n",
    "def DASH(lh,wps):\n",
    "    x = DASH_activation(lh,wps)\n",
    "    return 1.0/(1 + np.exp(-x)) \n",
    "\n",
    "def whitehill_activation(lh):\n",
    "    B_t = 0#(lh['alpha'] - lh['delta']) #Best Guess\n",
    "    return np.log(np.sum(np.power(lh['elapses']/3600.0,-.5)))+B_t\n",
    "\n",
    "def whitehill(lh):\n",
    "    x = whitehill_activation(lh)\n",
    "    return 1.0/(1 + np.exp(-x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison Between DASH and Whitehill Implementation of(ACT-R)\n",
    "\n",
    "I compare predictions of DASH vs Whitehill's ACT-R implementation on three different studying patterns:\n",
    "1. Crammed: A little bit of study 2 days before, lots of study 30 minutes before the test\n",
    "2. Spaced: Study sessions are spaced over different days\n",
    "3. Massed: Lots of Study 2 days before the test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness = [0,0,1,0,0,1,1,0,1,1,1]\n",
    "massed_short_h = [\n",
    "    day(12)+hour(7)+mins(50),\n",
    "    day(12)+hour(7)+mins(51),\n",
    "    day(12)+hour(7)+mins(52),\n",
    "    day(12)+hour(7)+mins(53),\n",
    "    day(12)+hour(7)+mins(54),\n",
    "    day(12)+hour(7)+mins(55),\n",
    "    day(12)+hour(7)+mins(56),\n",
    "    day(12)+hour(7)+mins(57),\n",
    "    day(12)+hour(7)+mins(58),\n",
    "    day(12)+hour(7)+mins(59),\n",
    "    day(12)+hour(8)+mins(0),\n",
    "]\n",
    "\n",
    "massed_long_h = [\n",
    "    day(11)+hour(7)+mins(50),\n",
    "    day(11)+hour(7)+mins(51),\n",
    "    day(11)+hour(7)+mins(52),\n",
    "    day(11)+hour(7)+mins(53),\n",
    "    day(11)+hour(7)+mins(54),\n",
    "    day(11)+hour(7)+mins(55),\n",
    "    day(11)+hour(7)+mins(56),\n",
    "    day(11)+hour(7)+mins(57),\n",
    "    day(11)+hour(7)+mins(58),\n",
    "    day(11)+hour(7)+mins(59),\n",
    "    day(11)+hour(8)+mins(0),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "spaced_short_h = [\n",
    "    day(2)+hour(0)+mins(0),\n",
    "    day(3)+hour(0)+mins(0),\n",
    "    day(4)+hour(0)+mins(0),\n",
    "    day(5)+hour(0)+mins(0),\n",
    "    day(6)+hour(0)+mins(0),\n",
    "    day(7)+hour(0)+mins(0),\n",
    "    day(8)+hour(0)+mins(0),\n",
    "    day(9)+hour(0)+mins(0),\n",
    "    day(10)+hour(0)+mins(0),\n",
    "    day(11)+hour(0)+mins(0),\n",
    "    day(12)+hour(0)+mins(0),\n",
    "]\n",
    "\n",
    "spaced_long_h = [\n",
    "    day(1)+hour(0)+mins(0),\n",
    "    day(2)+hour(0)+mins(0),\n",
    "    day(3)+hour(0)+mins(0),\n",
    "    day(4)+hour(0)+mins(0),\n",
    "    day(5)+hour(0)+mins(0),\n",
    "    day(6)+hour(0)+mins(0),\n",
    "    day(7)+hour(0)+mins(0),\n",
    "    day(8)+hour(0)+mins(0),\n",
    "    day(9)+hour(0)+mins(0),\n",
    "    day(10)+hour(0)+mins(0),\n",
    "    day(11)+hour(0)+mins(0),\n",
    "]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Interestingly ACT-R with constant decay predicts that massing is the best, but DASH predicts that spacing is the best. Values are printed out here as the chance the student gets the test item correct (from 0% to 100%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DASH Predictions\n",
      "Massed-Short-Retention\t: 73.06%\n",
      "Massed-Long-Retention\t: 88.88%\n",
      "Spaced-Short-Retention\t: 94.52%\n",
      "Spaced-Long-Retention\t: 92.53%\n",
      "---------------------------------\n",
      "Whitehill ACT-R (constant decay) Predictions\n",
      "Massed-Short-Retention\t: 95.72%\n",
      "Massed-Long-Retention\t: 69.08%\n",
      "Spaced-Short-Retention\t: 56.80%\n",
      "Spaced-Long-Retention\t: 50.63%\n"
     ]
    }
   ],
   "source": [
    "now = day(12)+hour(8)+mins(10)\n",
    "masses_short_lh =  gen_learner_history(now, massed_short_h, correctness,alpha=.05,delta=.1)\n",
    "massed_long_lh  =  gen_learner_history(now, massed_long_h,  correctness,alpha=.05,delta=.1)\n",
    "spaced_short_lh =  gen_learner_history(now, spaced_short_h, correctness,alpha=.05,delta=.1) \n",
    "spaced_long_lh  =  gen_learner_history(now, spaced_long_h,  correctness,alpha=.05,delta=.1) \n",
    "\n",
    "print(\"DASH Predictions\")\n",
    "print(\"Massed-Short-Retention\\t: %2.2f%%\" % (100 * DASH(masses_short_lh,window_profiles)))\n",
    "print(\"Massed-Long-Retention\\t: %2.2f%%\" % (100 * DASH(massed_long_lh,window_profiles)))\n",
    "print(\"Spaced-Short-Retention\\t: %2.2f%%\" % (100 * DASH(spaced_short_lh,window_profiles)))\n",
    "print(\"Spaced-Long-Retention\\t: %2.2f%%\" % (100 * DASH(spaced_long_lh,window_profiles)))\n",
    "print(\"---------------------------------\")\n",
    "print(\"Whitehill ACT-R (constant decay) Predictions\")\n",
    "print(\"Massed-Short-Retention\\t: %2.2f%%\" % (100 * whitehill(masses_short_lh)))\n",
    "print(\"Massed-Long-Retention\\t: %2.2f%%\" % (100 * whitehill(massed_long_lh)))\n",
    "print(\"Spaced-Short-Retention\\t: %2.2f%%\" % (100 * whitehill(spaced_short_lh)))\n",
    "print(\"Spaced-Long-Retention\\t: %2.2f%%\" % (100 * whitehill(spaced_long_lh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
