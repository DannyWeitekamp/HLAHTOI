{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.586979749566321"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def B_t(ts, d, B):\n",
    "    temp = np.power(ts,-d)\n",
    "    return np.sum(temp) + B\n",
    "\n",
    "def activation(t,ts,w,s,d,B):\n",
    "    return B_t(t+ts,d,B) +  np.sum(w.reshape(1,-1)*s)\n",
    "\n",
    "\n",
    "ts = np.array([4,3,2,1])\n",
    "w = np.array([1,1])\n",
    "s = np.array([[1.5,0.0],\n",
    "              [0.0,1.5]])\n",
    "\n",
    "d = .5\n",
    "B = 0\n",
    "t = 4\n",
    "\n",
    "activation(t,ts,w,s,d,B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84797322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danny/.local/lib/python3.6/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "df = pd.read_csv(\"../../theo_prediction_repo/data/assistments17/preprocessed_data.csv\",delimiter=\"\\t\")\n",
    "#df = df[df['user_id'] < 20]\n",
    "now = np.max(df[\"timestamp\"])\n",
    "print(now)\n",
    "\n",
    "def dws_to_ints(day_windows):\n",
    "    day_windows = np.array(day_windows)\n",
    "    \n",
    "def calc_features(df,windows=[1.0/24,1,7,30]):\n",
    "    for i in range(len(windows)+1):\n",
    "        df['lop_'+str(i)] = 0\n",
    "        df['ltc_'+str(i)] = 0\n",
    "        \n",
    "    windows = [0.0] + [86400.0*x for x in windows] + [float(\"inf\")]\n",
    "    gb = df.groupby([\"user_id\",\"skill_id\"])\n",
    "    for (u,s),g in gb:\n",
    "        #print(g)\n",
    "        np_ts = np.array(g['timestamp'])\n",
    "        np_corr = np.array(g['correct'])\n",
    "        diffs = np_ts.reshape(-1,1) - np_ts.reshape(1,-1)\n",
    "        for i in range(len(windows)-1):\n",
    "            upper = windows[i+1]\n",
    "            lower = windows[i]\n",
    "            in_win = np.logical_and(diffs > lower,diffs <= upper)\n",
    "            c_in_win = in_win * np_corr.reshape(1,-1)\n",
    "            df['lop_'+str(i)].iloc[g.index] = np.log(1+in_win.sum(axis=1))\n",
    "            df['ltc_'+str(i)].iloc[g.index] = np.log(1+c_in_win.sum(axis=1))\n",
    "         \n",
    "    return df\n",
    "df = calc_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id  item_id  timestamp  correct  skill_id     lop_0     ltc_0  \\\n",
      "0             0     1117    1049253        0       339  0.000000  0.000000   \n",
      "1             0     1118    1049302        1       339  0.693147  0.000000   \n",
      "2             0     1119    1049306        0       338  0.000000  0.000000   \n",
      "3             0     1119    1049312        0       338  0.693147  0.000000   \n",
      "4             0     1120    1049330        1        44  0.000000  0.000000   \n",
      "...         ...      ...        ...      ...       ...       ...       ...   \n",
      "934633     1707     1711   52026416        1       158  2.890372  1.386294   \n",
      "934634     1707     1712   52026420        0       158  2.944439  1.609438   \n",
      "934635     1707     1712   52026491        0       158  2.995732  1.609438   \n",
      "934636     1707     1712   52026495        1       158  3.044522  1.609438   \n",
      "934637     1707      835   52026498        1       174  0.000000  0.000000   \n",
      "\n",
      "        lop_1  ltc_1  lop_2  ltc_2  lop_3  ltc_3  lop_4  ltc_4  \n",
      "0         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "1         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "2         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "3         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "4         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "...       ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "934633    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "934634    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "934635    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "934636    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "934637    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
      "\n",
      "[934638 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "df.to_csv(\"a17_tw_windows.txt\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id  item_id  timestamp  correct  skill_id  op_0  tc_0  op_1  \\\n",
      "0             0     1117    1049253        0       339     0     0     0   \n",
      "1             0     1118    1049302        1       339     1     0     0   \n",
      "2             0     1119    1049306        0       338     0     0     0   \n",
      "3             0     1119    1049312        0       338     1     0     0   \n",
      "4             0     1120    1049330        1        44     0     0     0   \n",
      "...         ...      ...        ...      ...       ...   ...   ...   ...   \n",
      "934633     1707     1711   52026416        1       158    17     3     0   \n",
      "934634     1707     1712   52026420        0       158    18     4     0   \n",
      "934635     1707     1712   52026491        0       158    19     4     0   \n",
      "934636     1707     1712   52026495        1       158    20     4     0   \n",
      "934637     1707      835   52026498        1       174     0     0     0   \n",
      "\n",
      "        tc_1  op_2  tc_2  op_3  tc_3  op_4  tc_4  \n",
      "0          0     0     0     0     0     0     0  \n",
      "1          0     0     0     0     0     0     0  \n",
      "2          0     0     0     0     0     0     0  \n",
      "3          0     0     0     0     0     0     0  \n",
      "4          0     0     0     0     0     0     0  \n",
      "...      ...   ...   ...   ...   ...   ...   ...  \n",
      "934633     0     0     0     0     0     0     0  \n",
      "934634     0     0     0     0     0     0     0  \n",
      "934635     0     0     0     0     0     0     0  \n",
      "934636     0     0     0     0     0     0     0  \n",
      "934637     0     0     0     0     0     0     0  \n",
      "\n",
      "[934638 rows x 15 columns]\n",
      "(934638,) (934638, 12)\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "import pandas as pd\n",
    "n_skills = df[\"skill_id\"].max()+1\n",
    "n_students = df[\"user_id\"].max()+1\n",
    "\n",
    "Y = df[\"correct\"].to_numpy(dtype=np.float32)\n",
    "#[]'user_id','item_id',\n",
    "X = df.loc[:,~df.columns.isin(['item_id','timestamp','correct'])].to_numpy(dtype=np.float32)\n",
    "print(Y.shape,X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class DASH(torch.nn.Module):\n",
    "    def __init__(self,n_students, n_kcs, windows=[1/24,1,7,30],add_inf=True):\n",
    "        super(DASH, self).__init__()\n",
    "        self.windows = windows\n",
    "        self.n_wins = len(windows)+1 if(add_inf) else len(windows)\n",
    "        self.phi = nn.Parameter(torch.zeros(self.n_wins))\n",
    "        self.psi = nn.Parameter(torch.zeros(self.n_wins))\n",
    "        self.alpha = nn.Parameter(torch.zeros(n_students))\n",
    "        self.delta = nn.Parameter(torch.zeros(n_kcs))\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = 0\n",
    "        for i,w in enumerate(self.windows):\n",
    "            op = x[2+(i)*2]\n",
    "            tc = x[2+(i)*2+1]\n",
    "            s += self.phi[i]*torch.log(1+tc) - self.psi[i]*torch.log(1+op)\n",
    "        \n",
    "        return F.sigmoid(self.alpha[x[0].int()]-self.delta[x[1].int()],s) # self.alpha[x[0].int()]-self.delta[x[1].int()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411\n",
      "[1, 10000] loss: 0.671\n",
      "[1, 20000] loss: 0.667\n",
      "[1, 30000] loss: 0.643\n",
      "[1, 40000] loss: 0.665\n",
      "[1, 50000] loss: 0.658\n",
      "[1, 60000] loss: 0.648\n",
      "[1, 70000] loss: 0.666\n",
      "[1, 80000] loss: 0.641\n",
      "[1, 90000] loss: 0.638\n",
      "[1, 100000] loss: 0.651\n",
      "[1, 110000] loss: 0.647\n",
      "[1, 120000] loss: 0.648\n",
      "[1, 130000] loss: 0.646\n",
      "[1, 140000] loss: 0.656\n",
      "[1, 150000] loss: 0.658\n",
      "[1, 160000] loss: 0.657\n",
      "[1, 170000] loss: 0.651\n",
      "[1, 180000] loss: 0.651\n",
      "[1, 190000] loss: 0.642\n",
      "[1, 200000] loss: 0.665\n",
      "[1, 210000] loss: 0.643\n",
      "[1, 220000] loss: 0.639\n",
      "[1, 230000] loss: 0.651\n",
      "[1, 240000] loss: 0.658\n",
      "[1, 250000] loss: 0.654\n",
      "[1, 260000] loss: 0.648\n",
      "[1, 270000] loss: 0.648\n",
      "[1, 280000] loss: 0.662\n",
      "[1, 290000] loss: 0.651\n",
      "[1, 300000] loss: 0.657\n",
      "[1, 310000] loss: 0.652\n",
      "[1, 320000] loss: 0.644\n",
      "[1, 330000] loss: 0.634\n",
      "[1, 340000] loss: 0.651\n",
      "[1, 350000] loss: 0.654\n",
      "[1, 360000] loss: 0.668\n",
      "[1, 370000] loss: 0.664\n",
      "[1, 380000] loss: 0.647\n",
      "[1, 390000] loss: 0.667\n",
      "[1, 400000] loss: 0.660\n",
      "[1, 410000] loss: 0.664\n",
      "[1, 420000] loss: 0.650\n",
      "[1, 430000] loss: 0.653\n",
      "[1, 440000] loss: 0.648\n",
      "[1, 450000] loss: 0.656\n",
      "[1, 460000] loss: 0.646\n",
      "[1, 470000] loss: 0.644\n",
      "[1, 480000] loss: 0.664\n",
      "[1, 490000] loss: 0.658\n",
      "[1, 500000] loss: 0.647\n",
      "[1, 510000] loss: 0.635\n",
      "[1, 520000] loss: 0.662\n",
      "[1, 530000] loss: 0.662\n",
      "[1, 540000] loss: 0.665\n",
      "[1, 550000] loss: 0.646\n",
      "[1, 560000] loss: 0.654\n",
      "[1, 570000] loss: 0.649\n",
      "[1, 580000] loss: 0.649\n",
      "[1, 590000] loss: 0.669\n",
      "[1, 600000] loss: 0.635\n",
      "[1, 610000] loss: 0.644\n",
      "[1, 620000] loss: 0.665\n",
      "[1, 630000] loss: 0.655\n",
      "[1, 640000] loss: 0.671\n",
      "[1, 650000] loss: 0.651\n",
      "[1, 660000] loss: 0.654\n",
      "[1, 670000] loss: 0.654\n",
      "[1, 680000] loss: 0.643\n",
      "[1, 690000] loss: 0.643\n",
      "[1, 700000] loss: 0.653\n",
      "[1, 710000] loss: 0.649\n",
      "[1, 720000] loss: 0.654\n",
      "[1, 730000] loss: 0.660\n",
      "[1, 740000] loss: 0.659\n",
      "[1, 750000] loss: 0.641\n",
      "[1, 760000] loss: 0.654\n",
      "[1, 770000] loss: 0.654\n",
      "[1, 780000] loss: 0.663\n",
      "[1, 790000] loss: 0.663\n",
      "[1, 800000] loss: 0.666\n",
      "[1, 810000] loss: 0.659\n",
      "[1, 820000] loss: 0.642\n",
      "[1, 830000] loss: 0.655\n",
      "[1, 840000] loss: 0.656\n",
      "[1, 850000] loss: 0.658\n",
      "[1, 860000] loss: 0.664\n",
      "[1, 870000] loss: 0.654\n",
      "[1, 880000] loss: 0.648\n",
      "[1, 890000] loss: 0.649\n",
      "[1, 900000] loss: 0.668\n",
      "[1, 910000] loss: 0.652\n",
      "[1, 920000] loss: 0.648\n",
      "[1, 930000] loss: 0.660\n",
      "[2, 10000] loss: 0.668\n",
      "[2, 20000] loss: 0.666\n",
      "[2, 30000] loss: 0.643\n",
      "[2, 40000] loss: 0.665\n",
      "[2, 50000] loss: 0.658\n",
      "[2, 60000] loss: 0.648\n",
      "[2, 70000] loss: 0.666\n",
      "[2, 80000] loss: 0.641\n",
      "[2, 90000] loss: 0.638\n",
      "[2, 100000] loss: 0.651\n",
      "[2, 110000] loss: 0.646\n",
      "[2, 120000] loss: 0.648\n",
      "[2, 130000] loss: 0.646\n",
      "[2, 140000] loss: 0.656\n",
      "[2, 150000] loss: 0.658\n",
      "[2, 160000] loss: 0.656\n",
      "[2, 170000] loss: 0.651\n",
      "[2, 180000] loss: 0.651\n",
      "[2, 190000] loss: 0.642\n",
      "[2, 200000] loss: 0.665\n",
      "[2, 210000] loss: 0.643\n",
      "[2, 220000] loss: 0.639\n",
      "[2, 230000] loss: 0.651\n",
      "[2, 240000] loss: 0.658\n",
      "[2, 250000] loss: 0.654\n",
      "[2, 260000] loss: 0.648\n",
      "[2, 270000] loss: 0.648\n",
      "[2, 280000] loss: 0.662\n",
      "[2, 290000] loss: 0.651\n",
      "[2, 300000] loss: 0.657\n",
      "[2, 310000] loss: 0.652\n",
      "[2, 320000] loss: 0.644\n",
      "[2, 330000] loss: 0.634\n",
      "[2, 340000] loss: 0.651\n",
      "[2, 350000] loss: 0.654\n",
      "[2, 360000] loss: 0.668\n",
      "[2, 370000] loss: 0.664\n",
      "[2, 380000] loss: 0.647\n",
      "[2, 390000] loss: 0.667\n",
      "[2, 400000] loss: 0.660\n",
      "[2, 410000] loss: 0.664\n",
      "[2, 420000] loss: 0.650\n",
      "[2, 430000] loss: 0.653\n",
      "[2, 440000] loss: 0.648\n",
      "[2, 450000] loss: 0.656\n",
      "[2, 460000] loss: 0.646\n",
      "[2, 470000] loss: 0.644\n",
      "[2, 480000] loss: 0.664\n",
      "[2, 490000] loss: 0.658\n",
      "[2, 500000] loss: 0.647\n",
      "[2, 510000] loss: 0.635\n",
      "[2, 520000] loss: 0.662\n",
      "[2, 530000] loss: 0.662\n",
      "[2, 540000] loss: 0.665\n",
      "[2, 550000] loss: 0.646\n",
      "[2, 560000] loss: 0.654\n",
      "[2, 570000] loss: 0.649\n",
      "[2, 580000] loss: 0.649\n",
      "[2, 590000] loss: 0.669\n",
      "[2, 600000] loss: 0.635\n",
      "[2, 610000] loss: 0.644\n",
      "[2, 620000] loss: 0.665\n",
      "[2, 630000] loss: 0.655\n",
      "[2, 640000] loss: 0.671\n",
      "[2, 650000] loss: 0.651\n",
      "[2, 660000] loss: 0.654\n",
      "[2, 670000] loss: 0.654\n",
      "[2, 680000] loss: 0.643\n",
      "[2, 690000] loss: 0.643\n",
      "[2, 700000] loss: 0.653\n",
      "[2, 710000] loss: 0.649\n",
      "[2, 720000] loss: 0.654\n",
      "[2, 730000] loss: 0.660\n",
      "[2, 740000] loss: 0.659\n",
      "[2, 750000] loss: 0.641\n",
      "[2, 760000] loss: 0.654\n",
      "[2, 770000] loss: 0.654\n",
      "[2, 780000] loss: 0.663\n",
      "[2, 790000] loss: 0.663\n",
      "[2, 800000] loss: 0.666\n",
      "[2, 810000] loss: 0.659\n",
      "[2, 820000] loss: 0.642\n",
      "[2, 830000] loss: 0.655\n",
      "[2, 840000] loss: 0.656\n",
      "[2, 850000] loss: 0.658\n",
      "[2, 860000] loss: 0.664\n",
      "[2, 870000] loss: 0.654\n",
      "[2, 880000] loss: 0.648\n",
      "[2, 890000] loss: 0.649\n",
      "[2, 900000] loss: 0.668\n",
      "[2, 910000] loss: 0.652\n",
      "[2, 920000] loss: 0.648\n",
      "[2, 930000] loss: 0.660\n",
      "[3, 10000] loss: 0.668\n",
      "[3, 20000] loss: 0.666\n",
      "[3, 30000] loss: 0.643\n",
      "[3, 40000] loss: 0.665\n",
      "[3, 50000] loss: 0.658\n",
      "[3, 60000] loss: 0.648\n",
      "[3, 70000] loss: 0.666\n",
      "[3, 80000] loss: 0.641\n",
      "[3, 90000] loss: 0.638\n",
      "[3, 100000] loss: 0.651\n",
      "[3, 110000] loss: 0.646\n",
      "[3, 120000] loss: 0.648\n",
      "[3, 130000] loss: 0.646\n",
      "[3, 140000] loss: 0.656\n",
      "[3, 150000] loss: 0.658\n",
      "[3, 160000] loss: 0.656\n",
      "[3, 170000] loss: 0.651\n",
      "[3, 180000] loss: 0.651\n",
      "[3, 190000] loss: 0.642\n",
      "[3, 200000] loss: 0.665\n",
      "[3, 210000] loss: 0.643\n",
      "[3, 220000] loss: 0.639\n",
      "[3, 230000] loss: 0.651\n",
      "[3, 240000] loss: 0.658\n",
      "[3, 250000] loss: 0.654\n",
      "[3, 260000] loss: 0.648\n",
      "[3, 270000] loss: 0.648\n",
      "[3, 280000] loss: 0.662\n",
      "[3, 290000] loss: 0.651\n",
      "[3, 300000] loss: 0.657\n",
      "[3, 310000] loss: 0.652\n",
      "[3, 320000] loss: 0.644\n",
      "[3, 330000] loss: 0.634\n",
      "[3, 340000] loss: 0.651\n",
      "[3, 350000] loss: 0.654\n",
      "[3, 360000] loss: 0.668\n",
      "[3, 370000] loss: 0.664\n",
      "[3, 380000] loss: 0.647\n",
      "[3, 390000] loss: 0.667\n",
      "[3, 400000] loss: 0.660\n",
      "[3, 410000] loss: 0.664\n",
      "[3, 420000] loss: 0.650\n",
      "[3, 430000] loss: 0.653\n",
      "[3, 440000] loss: 0.648\n",
      "[3, 450000] loss: 0.656\n",
      "[3, 460000] loss: 0.646\n",
      "[3, 470000] loss: 0.644\n",
      "[3, 480000] loss: 0.664\n",
      "[3, 490000] loss: 0.658\n",
      "[3, 500000] loss: 0.647\n",
      "[3, 510000] loss: 0.635\n",
      "[3, 520000] loss: 0.662\n",
      "[3, 530000] loss: 0.662\n",
      "[3, 540000] loss: 0.665\n",
      "[3, 550000] loss: 0.646\n",
      "[3, 560000] loss: 0.654\n",
      "[3, 570000] loss: 0.649\n",
      "[3, 580000] loss: 0.649\n",
      "[3, 590000] loss: 0.669\n",
      "[3, 600000] loss: 0.635\n",
      "[3, 610000] loss: 0.644\n",
      "[3, 620000] loss: 0.665\n",
      "[3, 630000] loss: 0.655\n",
      "[3, 640000] loss: 0.671\n",
      "[3, 650000] loss: 0.651\n",
      "[3, 660000] loss: 0.654\n",
      "[3, 670000] loss: 0.654\n",
      "[3, 680000] loss: 0.643\n",
      "[3, 690000] loss: 0.643\n",
      "[3, 700000] loss: 0.653\n",
      "[3, 710000] loss: 0.649\n",
      "[3, 720000] loss: 0.654\n",
      "[3, 730000] loss: 0.660\n",
      "[3, 740000] loss: 0.659\n",
      "[3, 750000] loss: 0.641\n",
      "[3, 760000] loss: 0.654\n",
      "[3, 770000] loss: 0.654\n",
      "[3, 780000] loss: 0.663\n",
      "[3, 790000] loss: 0.663\n",
      "[3, 800000] loss: 0.666\n",
      "[3, 810000] loss: 0.659\n",
      "[3, 820000] loss: 0.642\n",
      "[3, 830000] loss: 0.655\n",
      "[3, 840000] loss: 0.656\n",
      "[3, 850000] loss: 0.658\n",
      "[3, 860000] loss: 0.664\n",
      "[3, 870000] loss: 0.654\n",
      "[3, 880000] loss: 0.648\n",
      "[3, 890000] loss: 0.649\n",
      "[3, 900000] loss: 0.668\n",
      "[3, 910000] loss: 0.652\n",
      "[3, 920000] loss: 0.648\n",
      "[3, 930000] loss: 0.660\n",
      "[4, 10000] loss: 0.668\n",
      "[4, 20000] loss: 0.666\n",
      "[4, 30000] loss: 0.643\n",
      "[4, 40000] loss: 0.665\n",
      "[4, 50000] loss: 0.658\n",
      "[4, 60000] loss: 0.648\n",
      "[4, 70000] loss: 0.666\n",
      "[4, 80000] loss: 0.641\n",
      "[4, 90000] loss: 0.638\n",
      "[4, 100000] loss: 0.651\n",
      "[4, 110000] loss: 0.646\n",
      "[4, 120000] loss: 0.648\n",
      "[4, 130000] loss: 0.646\n",
      "[4, 140000] loss: 0.656\n",
      "[4, 150000] loss: 0.658\n",
      "[4, 160000] loss: 0.656\n",
      "[4, 170000] loss: 0.651\n",
      "[4, 180000] loss: 0.651\n",
      "[4, 190000] loss: 0.642\n",
      "[4, 200000] loss: 0.665\n",
      "[4, 210000] loss: 0.643\n",
      "[4, 220000] loss: 0.639\n",
      "[4, 230000] loss: 0.651\n",
      "[4, 240000] loss: 0.658\n",
      "[4, 250000] loss: 0.654\n",
      "[4, 260000] loss: 0.648\n",
      "[4, 270000] loss: 0.648\n",
      "[4, 280000] loss: 0.662\n",
      "[4, 290000] loss: 0.651\n",
      "[4, 300000] loss: 0.657\n",
      "[4, 310000] loss: 0.652\n",
      "[4, 320000] loss: 0.644\n",
      "[4, 330000] loss: 0.634\n",
      "[4, 340000] loss: 0.651\n",
      "[4, 350000] loss: 0.654\n",
      "[4, 360000] loss: 0.668\n",
      "[4, 370000] loss: 0.664\n",
      "[4, 380000] loss: 0.647\n",
      "[4, 390000] loss: 0.667\n",
      "[4, 400000] loss: 0.660\n",
      "[4, 410000] loss: 0.664\n",
      "[4, 420000] loss: 0.650\n",
      "[4, 430000] loss: 0.653\n",
      "[4, 440000] loss: 0.648\n",
      "[4, 450000] loss: 0.656\n",
      "[4, 460000] loss: 0.646\n",
      "[4, 470000] loss: 0.644\n",
      "[4, 480000] loss: 0.664\n",
      "[4, 490000] loss: 0.658\n",
      "[4, 500000] loss: 0.647\n",
      "[4, 510000] loss: 0.635\n",
      "[4, 520000] loss: 0.662\n",
      "[4, 530000] loss: 0.662\n",
      "[4, 540000] loss: 0.665\n",
      "[4, 550000] loss: 0.646\n",
      "[4, 560000] loss: 0.654\n",
      "[4, 570000] loss: 0.649\n",
      "[4, 580000] loss: 0.649\n",
      "[4, 590000] loss: 0.669\n",
      "[4, 600000] loss: 0.635\n",
      "[4, 610000] loss: 0.644\n",
      "[4, 620000] loss: 0.665\n",
      "[4, 630000] loss: 0.655\n",
      "[4, 640000] loss: 0.671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 650000] loss: 0.651\n",
      "[4, 660000] loss: 0.654\n",
      "[4, 670000] loss: 0.654\n",
      "[4, 680000] loss: 0.643\n",
      "[4, 690000] loss: 0.643\n",
      "[4, 700000] loss: 0.653\n",
      "[4, 710000] loss: 0.649\n",
      "[4, 720000] loss: 0.654\n",
      "[4, 730000] loss: 0.660\n",
      "[4, 740000] loss: 0.659\n",
      "[4, 750000] loss: 0.641\n",
      "[4, 760000] loss: 0.654\n",
      "[4, 770000] loss: 0.654\n",
      "[4, 780000] loss: 0.663\n",
      "[4, 790000] loss: 0.663\n",
      "[4, 800000] loss: 0.666\n",
      "[4, 810000] loss: 0.659\n",
      "[4, 820000] loss: 0.642\n",
      "[4, 830000] loss: 0.655\n",
      "[4, 840000] loss: 0.656\n",
      "[4, 850000] loss: 0.658\n",
      "[4, 860000] loss: 0.664\n",
      "[4, 870000] loss: 0.654\n",
      "[4, 880000] loss: 0.648\n",
      "[4, 890000] loss: 0.649\n",
      "[4, 900000] loss: 0.668\n",
      "[4, 910000] loss: 0.652\n",
      "[4, 920000] loss: 0.648\n",
      "[4, 930000] loss: 0.660\n",
      "[5, 10000] loss: 0.668\n",
      "[5, 20000] loss: 0.666\n",
      "[5, 30000] loss: 0.643\n",
      "[5, 40000] loss: 0.665\n",
      "[5, 50000] loss: 0.658\n",
      "[5, 60000] loss: 0.648\n",
      "[5, 70000] loss: 0.666\n",
      "[5, 80000] loss: 0.641\n",
      "[5, 90000] loss: 0.638\n",
      "[5, 100000] loss: 0.651\n",
      "[5, 110000] loss: 0.646\n",
      "[5, 120000] loss: 0.648\n",
      "[5, 130000] loss: 0.646\n",
      "[5, 140000] loss: 0.656\n",
      "[5, 150000] loss: 0.658\n",
      "[5, 160000] loss: 0.656\n",
      "[5, 170000] loss: 0.651\n",
      "[5, 180000] loss: 0.651\n",
      "[5, 190000] loss: 0.642\n",
      "[5, 200000] loss: 0.665\n",
      "[5, 210000] loss: 0.643\n",
      "[5, 220000] loss: 0.639\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-4531ae096125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(outputs,y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "print(n_skills)\n",
    "net = DASH(n_students,n_skills)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "X = torch.from_numpy(X) if isinstance(X,np.ndarray) else X\n",
    "Y = torch.from_numpy(Y) if isinstance(Y,np.ndarray) else Y\n",
    "\n",
    "for epoch in range(1000):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    #for i, data in enumerate(trainloader, 0):\n",
    "    for i in range(len(Y)):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        \n",
    "        y,x = Y[i],X[i]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(x)\n",
    "        #print(outputs,y)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10000 == 9999:    # print every 10000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phi tensor([-0.1015,  0.1419,  0.1439,  0.1933,  0.0000])\n",
      "psi tensor([-0.1941,  0.1973,  0.0245,  0.0347,  0.0000])\n",
      "alpha tensor([ 0.1007,  0.2384, -0.5528,  ..., -0.6654, -0.5438,  0.1393])\n",
      "delta tensor([ 9.9189e-01,  7.9419e-01,  2.2436e-02, -1.7705e-01,  6.9785e-02,\n",
      "         4.8587e-01, -1.3249e-01,  5.5750e-01,  1.2272e+00,  5.9690e-01,\n",
      "         4.1889e-01,  8.9203e-01,  1.6421e-02, -8.9410e-02,  3.9388e-01,\n",
      "         1.0031e+00,  4.5097e-01, -2.8075e-01,  1.9283e-01, -4.8321e-01,\n",
      "        -4.1701e-01, -1.5378e-01, -4.4443e-02, -2.0668e-01, -1.5037e-01,\n",
      "        -7.0495e-01,  4.7350e-01,  3.0021e-01,  3.2610e-01,  5.8234e-01,\n",
      "         1.3569e-01,  5.6145e-01,  6.5499e-03,  2.0457e-01,  2.5157e-01,\n",
      "         4.8568e-01,  4.6275e-01,  1.8024e-01,  6.0212e-01,  1.2014e+00,\n",
      "         9.5882e-01, -2.5901e-02,  3.2690e-01,  4.7374e-01,  2.2910e-01,\n",
      "         5.9374e-01,  3.0172e-01,  2.5434e-01,  4.9535e-01,  3.4592e-01,\n",
      "        -1.0937e-01,  9.0618e-01,  9.8810e-01,  4.7392e-01,  1.0697e+00,\n",
      "         5.1348e-01,  2.6394e-01,  6.8227e-01,  5.9262e-01,  7.6778e-01,\n",
      "         4.9120e-01, -1.5542e-01, -1.5630e-03, -7.9701e-01, -3.7226e-01,\n",
      "         8.3770e-01,  3.3878e-01,  1.2822e-01,  2.8193e-01,  7.6469e-01,\n",
      "         1.8205e+00,  9.3961e-01,  3.6252e-01,  2.7667e-01,  3.9524e-01,\n",
      "        -1.0541e-01, -3.0758e-01, -6.5025e-02,  3.5912e-02,  4.4795e-01,\n",
      "         2.7451e-01,  8.7183e-01,  6.5914e-02, -6.7792e-01,  2.2512e-01,\n",
      "         2.7369e-01, -1.3728e-01,  9.6549e-01,  8.7799e-01, -3.7845e-01,\n",
      "         2.2202e-01,  5.0083e-02,  1.2893e+00, -6.2689e-01, -4.5709e-01,\n",
      "         2.3961e-02, -6.2865e-01, -1.5206e-01,  5.7705e-02,  1.8091e-01,\n",
      "        -4.1555e-01,  3.3171e-01, -3.4846e-01,  2.6861e-01,  4.6261e-02,\n",
      "         2.5334e-01,  6.5001e-01, -4.5395e-01,  3.5628e-01,  7.9630e-01,\n",
      "         7.8393e-01,  9.5283e-01,  8.7966e-01,  7.1748e-01,  3.5321e-01,\n",
      "        -4.4251e-02,  7.5378e-02,  1.5301e+00, -9.4523e-01, -1.7208e-01,\n",
      "         4.7924e-02,  1.0440e-01,  1.1223e-01,  9.6104e-01,  1.5463e-01,\n",
      "         4.0386e-01, -6.2594e-01,  1.1144e+00,  1.1412e-02,  1.3487e+00,\n",
      "         2.9683e-01,  6.0513e-02,  2.6518e-01,  1.3877e-01, -4.1062e-01,\n",
      "         9.0205e-01, -6.7435e-01,  5.6131e-01, -4.1254e-02,  5.8263e-01,\n",
      "         2.3870e-01, -8.1587e-01,  5.1643e-01, -8.9961e-01,  1.7108e-01,\n",
      "        -3.8288e-01, -8.6431e-02,  1.2489e-01,  6.7145e-01, -1.9527e-01,\n",
      "        -7.0176e-01,  9.4294e-01, -2.0050e-01,  4.4127e-01, -7.1841e-01,\n",
      "         7.6873e-01,  3.6813e-01,  1.7795e-01,  5.1835e-01, -8.3393e-01,\n",
      "         5.1662e-01,  9.0957e-01,  4.8279e-01,  1.8237e-01,  7.2895e-01,\n",
      "        -1.8866e-02,  1.6628e+00, -2.6546e-01,  8.6622e-01,  7.3372e-01,\n",
      "        -3.5570e-01,  3.1487e-01,  4.0914e-01,  5.8338e-01,  7.9509e-01,\n",
      "         5.5444e-01,  6.7605e-01,  5.1590e-01,  1.2903e+00,  3.8668e-01,\n",
      "         1.0181e+00,  1.9807e-01,  2.4924e-01, -4.3393e-02,  5.3336e-01,\n",
      "         4.9158e-01,  3.8263e-01,  1.2185e+00, -2.9918e-02,  2.0960e-01,\n",
      "         1.4633e-01,  3.9477e-01,  6.3555e-01,  8.1948e-01,  6.7999e-01,\n",
      "         2.6357e-01,  4.6440e-01, -9.9105e-01, -3.1867e-01, -3.5862e-01,\n",
      "         6.9927e-01,  1.1080e-01,  4.0729e-01,  1.7257e-01, -1.2787e-01,\n",
      "         2.2800e-01,  8.2764e-01,  1.8052e+00,  8.6249e-01,  3.4378e-01,\n",
      "         6.8544e-01,  6.5967e-01,  2.9277e-01, -6.4392e-02,  4.8131e-01,\n",
      "        -4.0835e-02,  5.5610e-01,  2.3100e-01,  4.3322e-01,  9.5840e-01,\n",
      "        -3.8056e-01, -2.1554e-01,  6.0650e-01,  1.5782e+00,  4.5259e-01,\n",
      "        -5.9203e-03, -2.1537e-01,  6.2933e-01,  2.1134e-01,  6.5042e-01,\n",
      "        -2.3011e-02,  5.1257e-02,  1.5569e-01,  1.0248e-01,  3.3221e-01,\n",
      "         2.9358e-01,  4.7801e-01, -3.7589e-01, -3.3993e-02,  1.5107e-01,\n",
      "         7.7720e-01,  9.7934e-01,  8.0504e-02,  1.8572e-01,  2.8532e-01,\n",
      "         5.5803e-01, -8.8778e-02,  3.8381e-01,  2.3002e-01,  1.2976e-01,\n",
      "         4.3275e-01,  5.3931e-01,  7.1460e-01,  1.1295e-03,  5.4858e-02,\n",
      "         5.6358e-01,  7.9334e-01,  4.1983e-01,  8.8084e-01,  5.1829e-01,\n",
      "         4.7064e-02, -4.4552e-02,  3.5880e-01,  4.8533e-01, -1.3812e-01,\n",
      "         1.0155e+00,  1.0978e-01, -7.1061e-01,  6.1449e-01,  5.4345e-02,\n",
      "         2.0417e-01,  3.6746e-01, -3.8988e-01, -3.4349e-01,  1.1516e+00,\n",
      "         8.1529e-01,  8.2734e-01, -6.7500e-01,  8.0793e-01, -2.1150e-03,\n",
      "         4.0844e-01,  2.7775e-03,  1.8703e-01, -4.0385e-01,  4.9335e-01,\n",
      "         3.0167e-01,  5.1870e-01,  1.1131e+00,  6.8976e-01,  6.3808e-01,\n",
      "         1.9582e-01,  2.7471e-01,  6.4499e-01,  1.1409e-01,  6.2378e-01,\n",
      "        -4.3880e-02,  1.1025e+00,  3.7359e-01,  7.8962e-01,  1.3697e+00,\n",
      "        -3.9888e-01,  4.3221e-01,  2.5685e-01,  1.4079e+00,  2.9910e-01,\n",
      "         4.0227e-01, -4.5921e-01,  4.4015e-01, -3.5277e-01,  3.6781e-01,\n",
      "         1.4722e+00, -4.0444e-01,  9.3140e-01, -2.0130e-01,  9.2219e-01,\n",
      "         6.6889e-01,  3.0209e-01, -9.5233e-02,  1.5611e+00,  1.2203e+00,\n",
      "        -5.5190e-01,  3.8438e-01,  5.0774e-01,  8.9332e-01,  7.9159e-01,\n",
      "         5.5978e-01,  8.8164e-01, -1.4577e+00,  3.1832e-01, -6.7594e-01,\n",
      "         3.6389e-01,  6.8153e-01,  6.3395e-01,  5.7046e-01,  1.0629e-01,\n",
      "        -1.0958e+00,  7.5811e-01,  7.7102e-01,  2.4038e-01,  3.8830e-01,\n",
      "         3.4497e-01, -1.2813e-01,  1.1697e+00,  5.0089e-01, -1.8924e-01,\n",
      "         9.0204e-01,  6.2814e-01,  7.7045e-01, -1.6188e-01,  1.0949e-02,\n",
      "         1.9472e-01,  7.9341e-01, -4.2645e-01,  5.9119e-01,  7.2964e-02,\n",
      "        -4.8673e-01,  9.4829e-01,  3.2400e-01, -2.6149e-01,  5.7814e-01,\n",
      "         1.2251e-01,  2.7550e-01,  5.2943e-02,  7.5939e-01, -2.6787e-02,\n",
      "         8.6057e-01,  4.1506e-01,  1.1466e+00,  3.2486e-01,  1.2173e+00,\n",
      "         1.0316e+00,  8.1350e-01,  9.3964e-01,  1.0522e+00,  8.9057e-01,\n",
      "         1.5566e-01, -5.7467e-01, -5.1443e-01,  1.3007e+00, -1.1292e+00,\n",
      "         8.7249e-01,  2.8372e-01,  1.9368e-01, -3.5623e-01,  5.4083e-01,\n",
      "        -9.4248e-01,  6.8805e-01,  5.5782e-01,  3.3311e-01,  1.5253e+00,\n",
      "         4.7917e-01, -3.4318e-02, -2.3104e-01, -1.4310e-01,  5.9720e-01,\n",
      "         6.0405e-01, -3.1806e-01,  6.3969e-02,  6.1937e-01,  6.1682e-01,\n",
      "         3.9142e-01, -8.1248e-01, -2.3359e-02,  1.0846e-01,  8.0214e-01,\n",
      "        -1.1599e+00, -2.8073e-01, -1.3326e-01, -6.5488e-01, -1.2808e+00,\n",
      "         3.7165e-01])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "#print(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_learner_history(now,times,correct,alpha=0.0,delta=0.0,windows=[1/24,1,7,30]):\n",
    "    times = np.array(times)\n",
    "    correct = np.array(correct)\n",
    "    elapse = now-times\n",
    "    windows = [0.0] + [86400.0*x for x in windows] + [float('inf')]\n",
    "    lh = {}\n",
    "    for i in range(len(windows)-1):\n",
    "        upper = windows[i+1]\n",
    "        lower = windows[i]\n",
    "        in_win = np.logical_and(elapse > lower,elapse <= upper)\n",
    "        c_in_win = in_win * correct\n",
    "        \n",
    "        lh['n_w'+str(i)] = in_win.sum()\n",
    "        lh['c_w'+str(i)] = c_in_win.sum()\n",
    "    lh[\"alpha\"] = alpha\n",
    "    lh[\"delta\"] = delta\n",
    "    return lh\n",
    "\n",
    "def day(x):\n",
    "    return x * 86400.0\n",
    "def hour(x):\n",
    "    return x * 3600\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Fit with torch---\n",
    "#phi tensor([-0.1015,  0.1419,  0.1439,  0.1933,  0.0000])\n",
    "#psi tensor([-0.1941,  0.1973,  0.0245,  0.0347,  0.0000])\n",
    "\n",
    "#---Fit with glmer (in R)----\n",
    "#    ltc_0      lop_0      ltc_1      lop_1      ltc_2      lop_2      ltc_3      lop_3      ltc_4  lop_4  \n",
    "# 0.038408  -0.011849   0.048038  -0.032408   0.033345  -0.024711  -0.003275   0.005272  -0.043829  0.033895  \n",
    "    \n",
    " \n",
    "window_profiles = [{\n",
    "    'scale' : 1/24,\n",
    "    'phi' : .038408,\n",
    "    'psi' : -0.011849\n",
    "},\n",
    "{\n",
    "    'scale' : 1,\n",
    "    'phi' : 0.048038,\n",
    "    'psi' : -0.032408\n",
    "},\n",
    "{\n",
    "    'scale' : 7,\n",
    "    'phi' : 0.033345,\n",
    "    'psi' : -0.024711\n",
    "},\n",
    "{\n",
    "    'scale' : 30,\n",
    "    'phi' : -0.003275,\n",
    "    'psi' : 0.005272\n",
    "},\n",
    "]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x)) \n",
    "\n",
    "def DASH_activation(lh,wps):\n",
    "    s = 0\n",
    "    for i,wp in enumerate(wps):\n",
    "        s += wp[\"phi\"]*np.log(1+lh[\"c_w\"+str(i)])-wp[\"psi\"]*np.log(1+lh[\"n_w\"+str(i)])\n",
    "    return lh['alpha'] - lh['delta'] + s\n",
    "\n",
    "def DASH(lh,wps):\n",
    "    x = DASH_activation(lh,wps)\n",
    "    return 1/(1 + np.exp(-x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_w0': 0, 'c_w0': 0, 'n_w1': 3, 'c_w1': 2, 'n_w2': 3, 'c_w2': 1, 'n_w3': 0, 'c_w3': 0, 'n_w4': 0, 'c_w4': 0, 'alpha': 0.05, 'delta': 0.3}\n",
      "0.4762857747894385\n"
     ]
    }
   ],
   "source": [
    "learner_history = gen_learner_history(day(2)+hour(1),[\n",
    "        day(0)+hour(0)+0,\n",
    "        day(0)+hour(0)+30,\n",
    "        day(0)+hour(0)+40,\n",
    "        day(1)+hour(3)+0,\n",
    "        day(1)+hour(3)+30,\n",
    "        day(1)+hour(3)+40,\n",
    "        ],[0,0,1,0,1,1],alpha=.05,delta=.3)\n",
    "print(learner_history)\n",
    "print(DASH(learner_history,window_profiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import load_npz, csr_matrix\n",
    "data = load_npz(\"../theo_prediction_repo/data/spanish/X-isscwatw.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
